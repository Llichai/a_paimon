# Paimon 源码理解快速指南

本指南帮助开发者快速理解 Apache Paimon 的架构、关键组件和核心算法，建议按顺序阅读。

---

## 第一部分：项目概览（15 分钟）

### 1.1 项目定位

**Apache Paimon** 是一个完整的**湖仓一体化存储引擎**，具有以下特点：
- 支持 OLAP 和 OLTP 混合工作负载
- 基于 LSM 树架构，优化数据写入性能
- 支持多种计算引擎集成（Flink、Spark、Hive）
- 支持主流云存储服务

### 1.2 核心价值

| 价值维度 | 具体内容 |
|---------|---------|
| **性能** | 低延迟写入、高效查询、自动压缩优化 |
| **功能** | 主键约束、实时 CDC、Schema 演化、MVCC |
| **生态** | 多引擎支持、多格式兼容、云原生设计 |
| **可维护性** | 清晰分层、接口隔离、完善文档 |

### 1.3 快速数字

- **总代码量**: 60+ 万行
- **Java 文件数**: 2,698 个
- **核心模块**: 3 个（api、common、core）
- **集成模块**: 5 大类（格式、引擎、存储、索引、服务）

---

## 第二部分：分层架构详解（30 分钟）

### 2.1 七层分层架构

```
第1层: 云存储接口（S3、OSS、COS、GCS、Azure、OBS、Jindo）
       ↑
第2层: 文件系统抽象（paimon-filesystems）
       ↑
第3层: 计算引擎集成（Flink、Spark、Hive）
       ↑
第4层: 打包层（paimon-bundle）
       ↑
第5层: 核心与服务层（paimon-core、paimon-service）
       ↑
第6层: 通用基础设施（paimon-common）
       ↑
第7层: 轻量 API 层（paimon-api）- 无外部依赖
```

### 2.2 核心三大模块

#### 📦 paimon-api（轻量级 API 层）
- **职责**: 定义接口契约，无具体实现
- **关键类**:
  - `CatalogFactory` - Catalog 工厂
  - `Table`, `View` - 表和视图接口
  - `Schema` - Schema 定义
  - `FileIndex` - 文件索引接口
- **特点**: 无 Paimon 内部依赖，避免引入重型库

#### 📦 paimon-common（通用基础设施）
- **职责**: 跨模块的公共功能
- **关键包**:
  - `data/` - 数据类型和行/列式数据结构
  - `fs/` - 文件系统抽象
  - `format/` - 数据格式编解码
  - `compression/` - 压缩算法
  - `predicate/` - 谓词表示和过滤
  - `statistics/` - 统计信息收集
  - `globalindex/` - 全局索引实现
- **规模**: 575 个 Java 文件

#### 📦 paimon-core（核心存储引擎）
- **职责**: 存储引擎实现，表的读写和维护
- **关键包**:
  - `mergetree/` - LSM 树实现
  - `compact/` - 压缩策略和合并函数
  - `manifest/` - Manifest 元数据管理
  - `snapshot/` - Snapshot 和版本管理
  - `catalog/` - Catalog 实现
  - `table/` - 表操作实现
  - `operation/` - 各类表操作（写入、更新、删除）
- **规模**: 767 个 Java 文件
- **存储后端**: RocksDB（用于状态存储）

### 2.3 分类关键模块

#### 格式支持
- `paimon-format` - ORC、Parquet、Avro
- `paimon-arrow` - Arrow 列式格式
- `paimon-lance` - 向量格式（AI/ML）

#### 计算引擎集成
- `paimon-flink` - Flink（1.16-2.2 多版本）
- `paimon-spark` - Spark（3.2-4.0 多版本）
- `paimon-hive` - Hive（2.1-3.1 多版本）

#### 索引与搜索
- `paimon-lucene` - 全文索引（Java 11+）
- `paimon-faiss` - 向量检索（AI/ML）

#### 数据迁移
- `paimon-iceberg` - Iceberg 互操作
- `paimon-hudi` - Hudi 互操作

---

## 第三部分：关键算法详解（60 分钟）

### 3.1 LSM 树压缩策略（最核心）

**概念**: 使用多层文件结构，定期压缩合并文件以优化查询性能

#### 🎯 UniversalCompaction - 通用压缩
**文件路径**: `paimon-core/mergetree/compact/UniversalCompaction.java`

**核心思想**: 按文件大小和比例选择压缩单元，平衡写放大、读放大、空间放大

**三大触发条件**（按优先级）:
```
1. 空间放大（Size Amplification）
   当: candidateSize * 100 > maxSizeAmp * earliestRunSize
   做: 全量压缩所有文件

2. 大小比例（Size Ratio）
   当: 连续文件大小满足比例条件
   做: 增量压缩大小相近的文件

3. 文件数量（File Num）
   当: 文件数 > numRunCompactionTrigger
   做: 强制压缩防止过多文件
```

**数学示例**:
```
maxSizeAmp = 200(允许200%额外空间)
earliestRunSize = 100MB
candidateSize = 250MB

检查: 250 * 100 > 200 * 100 → 25000 > 20000 ✓
结果: 触发全量压缩
```

**学习路径**:
1. 查看代码中的 `pick()` 方法了解三大条件的检查顺序
2. 理解 `pickForSizeRatio()` 中的文件选择算法
3. 对比 RocksDB 的实现理解优化思路

**参考链接**: [RocksDB Universal Compaction](https://github.com/facebook/rocksdb/wiki/Universal-Compaction)

---

#### 🎯 LoserTree - 败者树归并
**文件路径**: `paimon-core/mergetree/compact/LoserTree.java`

**核心问题**: 多个 `RecordReader` 包含重复的键，需要在正确时刻返回这些键

**特殊约束**: 对象复用问题
```
❌ 错误方式:
reader1.next()  // 返回 key1
reader2.next()  // 返回 key1
// 此时不能立即调用 reader1.next() 获取下一个键
// 因为第一个 key1 可能在 MergeFunction 中被复用

✅ 正确方式:
必须等所有 Reader 中的 key1 都返回后
才能调用任何 Reader 获取新键
```

**六种节点状态**:
```
胜者 + 新键 (WINNER_WITH_NEW_KEY)
胜者 + 相同键 (WINNER_WITH_SAME_KEY)
胜者 + 已弹出 (WINNER_POPPED)
败者 + 新键 (LOSER_WITH_NEW_KEY)
败者 + 相同键 (LOSER_WITH_SAME_KEY)
败者 + 已弹出 (LOSER_POPPED)
```

**性能优化**:
```
快速路径: 当胜者已被pop且存在相同键时
直接跳转到 firstSameKeyIndex
避免逐层比较，性能提升显著
```

**学习路径**:
1. 理解对象复用问题（这是核心难点）
2. 学习六种状态的定义和转换逻辑
3. 对比常规败者树理解优化设计
4. 查看 Wiki：[Paimon LoserTree Design](https://cwiki.apache.org/confluence/x/9Ak0Dw)

---

### 3.2 多维索引加速查询

#### 🎯 ZIndexer - Z-Order 索引
**文件路径**: `paimon-common/sort/zorder/ZIndexer.java`

**用途**: 将多维数据映射到一维，保留空间局部性

**实现原理**: 比特位交错
```
点(x=5, y=3)的二进制表示:
x = 101(二进制)
y = 011(二进制)

Z-Order索引: 100111 = 39
```

**关键优势**:
- 相邻的多维点在一维上也相邻
- 支持范围查询和聚类
- 实现简单，计算快速

---

#### 🎯 BTreeIndexWriter - B-Tree 全局索引
**文件路径**: `paimon-common/globalindex/btree/BTreeIndexWriter.java`

**用途**: 快速查询指定键对应的行号（rowId）

**文件格式** (从底向上):
```
━━━━━━━━━━━━━━━━━━━━━━━━━━━━
        Footer
    (索引偏移信息)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━
       Index 块
    (B-Tree索引)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    BloomFilter 块
   (快速键存在判断)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    NULL位图块
  (Roaring位图存储)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    数据块(多个)
   (实际键值对数据)
━━━━━━━━━━━━━━━━━━━━━━━━━━━━
```

**设计细节**:
- **键合并**: 相同键的多个 rowId 合并存储
- **变长编码**: rowId 使用 VarLenLong 压缩
- **NULL 分离**: NULL 键单独用位图存储
- **块级压缩**: 各部分独立压缩，灵活高效

---

### 3.3 数据合并与聚合

#### 🎯 PartialUpdateMergeFunction - 部分更新
**文件路径**: `paimon-core/mergetree/compact/PartialUpdateMergeFunction.java`

**应用场景**: 维度表更新、渐变维处理

**核心概念**: 序列组（Sequence Group）
```
每个字段可关联不同的序列号字段:

字段        序列号字段
────────────────────
name   →   seq1
age    →   seq2
city   →   seq3

更新时独立比较序列号，选择最新版本
```

**删除策略** (4种):
```
1. 默认拒绝   - 遇到DELETE记录则抛异常
2. ignore-delete - 忽略DELETE，只保留有效数据
3. remove-record-on-delete - DELETE记录清空整行
4. 序列组部分删除 - 按序列号决定字段是否删除
```

---

### 3.4 查询优化

#### 🎯 PredicateConverter - 谓词下推
**文件路径**: `paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/PredicateConverter.java`

**目标**: 将计算引擎的查询条件转换为 Paimon 的谓词，提前过滤数据

**支持的表达式** (转换规则矩阵):
```
Flink 表达式      →  Paimon 谓词
═══════════════════════════════
EqualTo(col, v)   →  col = v
NotEqualTo(col, v) → col != v
LessThan(col, v)  →  col < v
GreaterThan(col, v) → col > v
In(col, [v1,v2])  →  col IN (v1, v2)
Like(col, "%abc%") → col LIKE "%abc%"
IsNull(col)       →  col IS NULL
```

**特殊处理**: LIKE 模式转换
```
LIKE "%abc%"    →  Contains("abc")
LIKE "abc%"     →  StartsWith("abc")
LIKE "%abc"     →  EndsWith("abc")
```

---

## 第四部分：源码阅读地图（按难度）

### 📍 入门级（新手友好）

1. **类型系统**
   - 文件: `paimon-common/data/InternalRow.java`
   - 难度: ⭐
   - 时间: 15 分钟
   - 内容: 数据类型定义，行式存储

2. **文件系统接口**
   - 文件: `paimon-common/fs/FileIO.java`
   - 难度: ⭐
   - 时间: 20 分钟
   - 内容: 文件系统抽象

3. **Schema 定义**
   - 文件: `paimon-api/schema/Schema.java`
   - 难度: ⭐
   - 时间: 15 分钟
   - 内容: Schema 结构和演化

---

### 📍 中等级（理解原理）

1. **UniversalCompaction**
   - 文件: `paimon-core/mergetree/compact/UniversalCompaction.java`
   - 难度: ⭐⭐⭐
   - 时间: 2 小时
   - 先学: LSM 树基础、文件压缩概念

2. **LoserTree**
   - 文件: `paimon-core/mergetree/compact/LoserTree.java`
   - 难度: ⭐⭐⭐⭐
   - 时间: 2 小时
   - 先学: 败者树算法、对象复用问题

3. **BloomFilter**
   - 文件: `paimon-common/utils/BloomFilter.java`
   - 难度: ⭐⭐
   - 时间: 1 小时
   - 先学: 哈希函数、概率数据结构

---

### 📍 高等级（深度研究）

1. **PartialUpdateMergeFunction**
   - 文件: `paimon-core/mergetree/compact/PartialUpdateMergeFunction.java`
   - 难度: ⭐⭐⭐⭐⭐
   - 时间: 3-4 小时
   - 先学: LSM 合并、状态管理、序列号机制

2. **SnapshotReaderImpl**
   - 文件: `paimon-core/table/source/snapshot/SnapshotReaderImpl.java`
   - 难度: ⭐⭐⭐⭐⭐
   - 时间: 4-5 小时
   - 先学: Snapshot 概念、MVCC、增量读取

3. **PredicateConverter**
   - 文件: `paimon-flink/paimon-flink-common/src/main/java/org/apache/paimon/flink/PredicateConverter.java`
   - 难度: ⭐⭐⭐⭐
   - 时间: 2-3 小时
   - 先学: Flink 谓词表示、表达式树

---

## 第五部分：常见问题解答（FAQ）

### Q1: Paimon 与 Iceberg/Delta 有什么区别？

**A**: 核心差异如下：

| 特性 | Paimon | Iceberg | Delta |
|------|--------|---------|-------|
| 写入延迟 | 毫秒级 | 秒级+ | 秒级+ |
| 存储格式 | LSM 树 | 快照 | Delta log |
| 压缩开销 | 后台自动 | 手动触发 | 自动合并 |
| CDC 支持 | 原生 | 需扩展 | 原生 |
| 主键约束 | 支持 | 不支持 | 支持 |

**结论**: Paimon 针对**低延迟写入 + 高效查询**优化，Iceberg 针对**完整快照 + 版本管理**优化。

---

### Q2: 为什么使用 LSM 树而不是 B+ 树？

**A**: LSM 树优势：
- **顺序写入**: 所有写操作都是顺序的，充分利用磁盘带宽
- **批量刷新**: 数据批量刷到磁盘，减少 I/O 次数
- **并发友好**: 无锁写入设计，天然支持高并发

B+ 树劣势：
- **随机写入**: 每次更新都是随机定位，效率低
- **高维护成本**: 需要复杂的平衡机制

---

### Q3: 压缩策略如何选择？

**A**: Paimon 使用 Universal Compaction，基于三个因素动态选择：

```
优先级 1: 空间放大
  → 当额外空间超过阈值，触发全量压缩

优先级 2: 大小比例
  → 按文件大小比例，合并大小相近的文件

优先级 3: 文件数量
  → 文件过多时强制压缩，防止查询性能下降
```

**最佳实践**:
- 小表（<100GB）: 使用默认参数
- 中等表（100GB-1TB）: 调整 sizeRatio 参数
- 大表（>1TB）: 启用 offPeakHours 非峰值压缩

---

### Q4: 如何理解"对象复用"问题？

**A**: 这是 LoserTree 的核心难点：

```
问题场景:
Reader1: [key1-v1, key1-v2, key2]
Reader2: [key1-v3, key3]

读取顺序应该是: key1-v1 → key1-v2 → key1-v3 → key2 → key3

但如果我们这样做:
kv1 = reader1.next()  // key1-v1
kv2 = reader2.next()  // key1-v3
kv3 = reader1.next()  // key1-v2 或 key2?

问题: reader1 返回的对象可能在 MergeFunction 中被复用
因此不能假设 kv1 还持有原始值!

解决方案:
必须等所有 Reader 的相同键都被消费后
才能读取新键
```

---

## 第六部分：学习路线建议

### 🎯 1周快速入门

| 天数 | 内容 | 时间 |
|------|------|------|
| 第1天 | 项目结构、分层架构 | 2h |
| 第2天 | LSM 树和压缩基础 | 3h |
| 第3天 | UniversalCompaction 源码 | 3h |
| 第4天 | 数据类型和行列式 | 2h |
| 第5天 | Snapshot 和版本管理 | 3h |

### 🎯 1个月深度学习

**第1周**: 基础概念（如上）
**第2周**:
- LoserTree 算法和实现
- 各种合并函数
- 索引设计（Z-Order、BTree）

**第3周**:
- 查询优化（谓词下推、分区剪枝）
- 计算引擎集成（以 Flink 为例）

**第4周**:
- 分布式问题（并发控制、事务）
- 性能优化和调优

---

## 第七部分：参考资源

### 📚 官方文档
- [Paimon 项目主页](https://paimon.apache.org/)
- [Paimon 文档](https://paimon.apache.org/docs/)
- [GitHub 源码仓库](https://github.com/apache/incubator-paimon)

### 📄 参考论文
- [Google BigTable](https://research.google.com/pubs/pub27898.html)
- [Leveldb](https://github.com/google/leveldb)
- [RocksDB](https://rocksdb.org/)

### 📺 相关技术
- LSM 树: https://en.wikipedia.org/wiki/Log-structured_merge-tree
- 布隆过滤器: https://en.wikipedia.org/wiki/Bloom_filter
- Z-Order 曲线: https://en.wikipedia.org/wiki/Z-order_curve

---

## 快速导航

| 需求 | 参考文档 |
|------|---------|
| 了解项目全貌 | `项目结构分析.md` |
| 学习关键算法 | `算法难点详解.md`、本文件 |
| 规范代码注释 | `注释规范文档.md` |
| 查看改进进度 | `注释进度日志.txt` |
| 快速定位代码 | `修改文件列表.txt` |

---

**最后更新**: 2026年2月12日
**适用版本**: Apache Paimon 1.4+
**难度等级**: 中等以上（需要数据库和算法基础）
